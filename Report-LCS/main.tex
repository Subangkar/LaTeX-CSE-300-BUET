\documentclass{report}
    \usepackage[utf8]{inputenc}
    \usepackage{array}
    \usepackage{tikz}
    \usepackage{color}
    \usepackage{hyperref}
    % \usepackage{algorithm}
    \usepackage{algorithmic}
    \usepackage{listings}
    \usepackage{amsmath}
    % \usepackage{algpseudocode}
    \usepackage[linesnumbered,noline,noend]{algorithm2e}
    \usepackage{subfiles}
    
    \newcommand{\RNum}[1]{\uppercase\expandafter{\romannumeral #1\relax}}
    \newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
    \newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
    \newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
    
    \title{
    \line(1,0){300}
    \endgraf\bigskip
    \Huge
    % \bfseries{A Report on}
    % \endgraf\bigskip
    \emph{Longest Common Subsequence}
    \newline
    \line(1,0){300}
    \bigskip
    \bigskip
    }
    
    \author{
    \Large{Subangkar Karmaker}\\
    \Large{Student ID : 1505015}
    }
    
    \date{
    \endgraf\bigskip
    \Large{\today}
    }
    
    \newcommand{\TripleRowDoubleCol}[6]{
        \begin{center}
        \begin{tabular}{|c|c|}
         \hline
         #1 & #2\\\hline
         #3 & #4\\\hline
         #5 & #6\\\hline
        \end{tabular}
        \end{center}
    }
    
    \begin{document}
    
    \maketitle
    \renewcommand{\familydefault}{\sfdefault}
    
    \tableofcontents
    
    \chapter{Introduction}
    This report briefly illustrates the well known problem \textit{Longest Common Subsequence}. Longest Common Subsequence, in short LCS is the problem of finding the common subsequence of some sequences that has the longest length among all other subsequences of them.
    
    \bigskip
    Basically in LCS, we find longest common subsequence of two string. We can also find longest common subsequence of more than two strings but that is much more complicated. So here we will limit out discussion to finding longest common subsequence of two string.
    
    \bigskip
    So in this problem we will have two string as input and our output will be a single string that is the longest common subsequence of the above two.  
    \newpage
    
    
    \chapter{Subsequence}{\label{chap:subseq}}
    % \section*{1. Form the possessive singular of nouns with ’s}
    % \addcontentsline{toc}{section}{1. Form the possessive singular of nouns with ’s}
    \section{Subsequence}{
    \bigskip
    \subsection{Definition:}
    Given two sequences $X$ = $\langle$ $x_1$, $x_2$,...,$x_m$ $\rangle$ and $Z$  = $\langle$ $z_1$, $z_2$,...,$z_k$ $\rangle$, we say that $Z$ is a subsequence of $X$ if there is a strictly increasing sequence of $k$ indices $i_1$, $i_2$,...,$i_k$  $(1$ $\leq$ $i_1$ $<$ $i_2$ $<$ ... $<$ $i_k$ $\leq$ $m$) such that $Z$ = $\langle$ $x_{i_1}$, $x_{i_2}$,..., $x_{i_k}$ $\rangle$.\\
    
    Informally sequential elements in subsequence must have strictly increasing sequence in the original sequence.
    \bigskip
    }
    
    \subsection{Example of Subsequences:}{
    \bigskip
    \flushleft{Let's consider a string $X=$ $\langle$BEGINNING$\rangle$\\}
    \centering{
    \TripleRowDoubleCol{\textbf{Main Sequence}}{$\langle$BEGINNING$\rangle$}
    {\textbf{Subsequences}}{$\langle$BGN$\rangle$, $\langle$INNING$\rangle$, $\langle$BIS$\rangle$ etc.}
    {\textbf{Not Subsequence}}{~$\langle$EBG$\rangle$, $\langle$NINNIG$\rangle$, $\langle$BGL$\rangle$ etc.}
    }
    \bigskip
    \flushleft{Let's consider another string $Y=$ $\langle$BACDB$\rangle$\\}
    \centering{
    \TripleRowDoubleCol{\textbf{Main Sequence}}{$\langle$BACDB$\rangle$}
    {\textbf{Subsequences}}{$\langle$BDB$\rangle$, $\langle$CDB$\rangle$, $\langle$BCB$\rangle$ etc.}
    {\textbf{Not Subsequence}}{~$\langle$DCA$\rangle$, $\langle$BDA$\rangle$, $\langle$CDA$\rangle$ etc.}
    }}
    
    \newpage
    
    \section{Longest Common Subsequence}{
    \bigskip
    A longest common subsequence is any string that is of the longest length among any common subsequences. Obviously there can be more than on subsequence with the longest length. In that case any of those subsequences is a optimal solution by definition. Here are some examples,
    \begin{enumerate}
        \bigskip
        \item {\textbf{Original Sequences:} $S$ = $\langle$BDCB$\rangle$,  $T$ = $\langle$BACDB$\rangle$}
            \begin{itemize}
                \item {\textbf{Common Subsequences:} $\langle$B$\rangle$, $\langle$BD$\rangle$, $\langle$BCB$\rangle$ etc.}
                \item {\textbf{Longest Common Subsequence:} $\langle$BCB$\rangle$}
            \end{itemize}
        \bigskip
        \item {\textbf{Original Sequences:}} $S$ = $\langle$DABKC$\rangle$,  $T$ = $\langle$APBCK$\rangle$
            \begin{itemize}
                \item \textbf{Common Subsequences:} $\langle$A$\rangle$, $\langle$AB$\rangle$, $\langle$ABK$\rangle$, $\langle$ABC$\rangle$ etc.
                \item \textbf{Longest Common Subsequence:} $\langle$ABK$\rangle$, $\langle$ABC$\rangle$
            \end{itemize}
        \bigskip
        \item {\textbf{Original Sequences:}} $S$ = $\langle$ABCD$\rangle$,  $T$ = $\langle$PQRS$\rangle$
            \begin{itemize}
                \item \textbf{Common Subsequences:} NO common subsequences
                \item \textbf{Longest Common Subsequence:} NO longest common subsequences
            \end{itemize}
    \end{enumerate}    
        
    
    }
    
    \newpage
    
    
    \chapter{Problem Solving Techniques}
    \bigskip
    There are many well established methodology and algorithms for problem solving.
    Brute force technique, Divide and conquer, Dynamic Programming are some popular examples of such methodologies. Here we will discuss Brute force technique and Dynamic Programming approach as both of them are relevant to our problem.
    \bigskip
    \section{Brute Force Technique}
    Brute force is a type of algorithm that tries a large number of patterns to solve a problem. It is often mentioned as \textit{Brute Force Search} or \textit{Exhaustive Search}. Brute Force Search is a very general problem-solving technique that consists of systematically enumerating all possible candidates for the solution and checking whether each candidate satisfies the problem's statement.\\
    For example, a brute-force algorithm to find the divisors of a natural number n would enumerate all integers from 1 to n, and check whether each of them divides n without remainder. A brute-force approach for the eight queens puzzle would examine all possible arrangements of 8 pieces on the 64-square chessboard, and, for each arrangement, check whether each (queen) piece can attack any other.
    
    \bigskip
    While a brute-force search is simple to implement, and will always find a solution if it exists, its cost is proportional to the number of candidate solutions – which in many practical problems tends to grow very quickly as the size of the problem increases. Therefore, brute-force search is typically used when the problem size is limited, or when there are problem-specific heuristics that can be used to reduce the set of candidate solutions to a manageable size. The method is also used when the simplicity of implementation is more important than speed.
    
    \subsection*{Basic algorithm}
    In order to apply brute-force search to a specific class of problems, one must implement four procedures, first,next, valid, and output. These procedures should take as a parameter the data P for the particular instance of the problem that is to be solved, and should do the following:
    
    \begin{enumerate}
        \item first (P): generate a first candidate solution for P.
        \item next (P, c): generate the next candidate for P after the current one c.
        \item valid (P, c): check whether candidate c is a solution for P.
        \item output (P, c): use the solution c of P as appropriate to the application.
    \end{enumerate}
    
    The next procedure must also tell when there are no more candidates for the instance P, after the current one c. A convenient way to do that is to return a "null candidate", some conventional data value $\land$ that is distinct from any real candidate. Likewise the first procedure should return $\land$ if there are no candidates at all for the instance P. The brute-force method is then expressed by the algorithm.
    ???????\\
    ???????\\
    ???????
    % \begin{algorithmic}
    % \State $c \gets first(P)$
    % \While{ $c \neq \land$}{
    %     \IF{valid(P,c)}{output(P, c)\;}
    %     \State $c \gets next(P,c)$
    % }
    % \end{algorithmic}
    
    \newpage
    \section{Dynamic Programming Approach}
    \bigskip
    Dynamic programming is both a mathematical optimization method and a computer programming method. The method was developed by Richard Bellman in the 1950s.
    In both contexts it refers to simplifying a complicated problem by breaking it down into simpler sub-problems in a recursive manner. While some decision problems cannot be taken apart this way, decisions that span several points in time do often break apart recursively. Likewise, in computer science, if a problem can be solved optimally by breaking it into sub-problems and then recursively finding the optimal solutions to the sub-problems, then it is said to have optimal substructure.
    \bigskip
    \subsection{Applicability for DP}
    There are two key attributes that a problem must have in order for dynamic programming to be applicable. Those are 
    \begin{itemize}
        \item {Optimal Substructure} 
        \item {Overlapping Sub-Problems}
    \end{itemize}
    If a problem can be solved by combining optimal solutions to non-overlapping sub-problems, the strategy is called "divide and conquer" instead of dynamic programming. This is why merge sort and quick sort are not classified as dynamic programming problems.
    
    \subsection{Optimal Substructure}
    Optimal substructure means that the solution to a given optimization problem can be obtained by the combination of optimal solutions to its sub-problems. Such optimal substructures are usually described by means of recursion.
    \\
    For example, given a graph G=(V,E), the shortest path p from a vertex u to a vertex v exhibits optimal substructure: take any intermediate vertex w on this shortest path p. If p is truly the shortest path, then it can be split into sub-paths p1 from u to w and p2 from w to v such that these, in turn, are indeed the shortest paths between the corresponding vertices (by the simple cut-and-paste argument described in ??????????????????????????????? Introduction to Algorithms). Hence, one can easily formulate the solution for finding shortest paths in a recursive manner, which is what the Bellman–Ford algorithm or the Floyd–Warshall algorithm does.
    \newpage
    \subsection{Overlapping Sub-Problems}
    \bigskip
    Overlapping sub-problems means that the space of sub-problems must be small, that is, any recursive algorithm solving the problem should solve the same sub-problems over and over, rather than generating new sub-problems.\\
    For example, consider the recursive formulation for generating the Fibonacci series: $F_i = F_{i{-}1} + F_{i{-}2}$, with base case $F_1 = F_2 = 1$. Then $F_{43} = F_{42} + F_{41}$, and $F_{42} = F_{41} + F_{40}$. Now $F_{41}$ is being solved in the recursive sub-trees of both $F_{43}$ as well as $F_{42}$. Even though the total number of sub-problems is actually small (only 43 of them), we end up solving the same problems over and over if we adopt a naive recursive solution such as this. Dynamic programming takes account of this fact and solves each sub-problem only once.
    This can be achieved in either of two ways:
    \bigskip
    \begin{itemize}
        \item {\textbf{Top-down approach:}} This is the direct fall-out of the recursive formulation of any problem. If the solution to any problem can be formulated recursively using the solution to its sub-problems, and if its sub-problems are overlapping, then one can easily memoize or store the solutions to the sub-problems in a table. Whenever we attempt to solve a new sub-problem, we first check the table to see if it is already solved. If a solution has been recorded, we can use it directly, otherwise we solve the sub-problem and add its solution to the table. 
        \bigskip
        \item {\textbf{Bottom-up approach:}} Once we formulate the solution to a problem recursively as in terms of its sub-problems, we can try reformulating the problem in a bottom-up fashion: try solving the sub-problems first and use their solutions to build-on and arrive at solutions to bigger sub-problems. This is also usually done in a tabular form by iteratively generating solutions to bigger and bigger sub-problems by using the solutions to small sub-problems. For example, if we already know the values of $F_{41}$ and $F_{40}$, we can directly calculate the value of $F_{42}$.
    \end{itemize}
    
    \chapter{Solving LCS: Brute Force Technique}
    \section{Solution}
    Suppose we are to find LCS of string X and Y.
    Then we can enumerate all subsequences of X, and check whether they are subsequences of Y and then take the longest one among them.
    \section{Time Complexity}
    Suppose X has length n and Y has length m.
    \begin{itemize}
        \item Checking = $\mathcal{O}(n)$ time per subsequence.
        \item $2^m$ subsequences of Y (each bit-vector of length m determines a distinct subsequence of Y).
    \end{itemize}
    Worst-case running time = $\mathcal{O}(n2^m) = $ \textcolor{red}{\textbf{exponential time}}.
    
    \section{Remarks}
    Although we can go with this algorithm for smaller values of m and n, we will not be able to solve this problem for strings having larger length i.e 100. So we need faster algorithms. Hence dynamic programming is in preference.
    
    
    
    
    \chapter{Solving LCS: Dynamic Programming Approach}
    We already know the properties that are required for a problem to be eligible to be solved with dynamic programming . LCS exhibits both Optimal substructure and Overlapping sub-problems property. Hence we can use dynamic programming to solve it. Now we will discuss the method of solving LCS with dynamic programming in detail.
    \section{Stucture of LCS problem}
    The structure of LCS can be represented as below:
    Suppose $X=\langle x_1x_2..x_m\rangle$ , $Y = \langle y_1y_2...y_n\rangle$ and $Z=\langle z_1 z_2...z_k\rangle$ is their longest commmon subsequence.\\
    Now we have:
    \begin{enumerate}
        \item If $x_m$ = $y_n$ then $z_k$ = $x_m$ = $y_n$. So $Z_{k-1}$ is an LCS of $X_{m-1}$ and $Y_{n-1}$.
        \item If $x_n$ $\neq$ $y_m$ and $z_k$ $\neq$ $x_m$ then $Z_k$ is an LCS of $X_{m-1}$ and $Y_n$.
        \item If $x_n$ $\neq$ $y_m$ and $z_k$ $\neq$ $y_n$ then $Z_k$ is an LCS of $X_{m}$ and $Y_{n-1}$.
    \end{enumerate}
    \section{Recursive Structure of the Subproblem}
    From the optimal substructure stated in the previous subsection, we know that, to find the LCS of X and Y, we just need to go through the following procedure:
    \begin{itemize}
        \item if $x_n$ = $y_m$, find the LCS of $X_{m-1}$ and $Y_{n-1}$ and then append $x_m$ (or $y_n$).
        \item if $x_n$ $\neq$ $y_m$, find the LCS of $X_{m-1}$ and Y, and the LCS of X and $Y_{n-1}$, then pick the longer one.
    \end{itemize}
    
    We can see a overlapping subproblems attribute from this recursive structure. For example, when finding the LCS of X and Y, we may need to find the LCS of X and $Y_{n-1}$ and the LCS of $X_{m-1}$ and Y first; and these two both depend on one subproblem, to find the LCS of $X_{m-1}$ and $Y_{n-1}$.
    
    So let us build the recursive relations among the optimal values of the subproblems.
    Denote c[i,j] as the length of $X_i$ and $Y_j$. When i=0 or j=0, the LCS of $X_i$ and $Y_j$ is an empty sequence, so c[i,j]=0, otherwise the recursive relation can be defined as:
        \[ c[i,j] =  \begin{cases} 
          0 &  \text{if $i=0$ or $j=0$}   \\
          c[i-1, j-1] & \text{if $i,j > 0$ and $x_i$ = $y_j$ } \\
          max(c[i-1, j], c[i, j-1]) & \text{if $i,j > 0$ and $x_i$  $\neq$ $y_j$ } 
       \end{cases} \]
    
    \section{Computing length of the LCS}
    Using the recursive formula defined above, we can easily contrive an algorithm to compute c[i,j], but the execution time will grow exponentially with the length of input. Since there are only $\Theta(m*n)$ subproblems in the subproblem space, we can use the bottom-up approach to improve efficiency.
    
    \bigskip
    The LCS\_LENGTH(X,Y) algorithm takes $X=\langle x_1x_2..x_m\rangle$ and $Y = \langle y_1y_2...y_n\rangle$ as inputs and then outputs two matrices c[0..m, 0..n] and b[1..m, 1..n]. c[i,j] stores the length of LCS($X_i$, $Y_j$) and b[i,j] stores where c[i,j] gets its value from (this will be explained later). At the end of the algorithm, the length of LCS(X,Y) will be stored at c[m,n].
    
    \subfile{LCS_LENGTH.tex}
    \newpage
    \section{Construct the LCS}
    With the help of matirx b from LCS\_LENGTH, we can construct the LCS of X and Y. Starting from b[m,n], we can navigate the matrix according to the direction of each 'arrow':
    \begin{itemize}
        \item when b[i,j] = \nwarrow,~it~means~$x_i=y_i$ is an element of LCS($X_i,Y_j$) i.e. LCS($X_i,Y_j$) is LCS(X$_{i-1}$,Y$_{j-1}$) appends $x_i$ (or $y_j$)
        \item when b[i,j] = \uparrow,~it~means~LCS($X_i,Y_j$) is the same as LCS(X$_{i-1},Y_j$)
        \item when b[i,j] = \leftarrow,~it~means~LCS($X_i,Y_j$) is the same as LCS(X$_i$,Y$_j$)
    \end{itemize}
    \subfile{PRINT-LCS.tex}
    E.g., for two sequences X=$\langle$ADAPT$\rangle$ and Y=$\langle$DBPT$\rangle$, the results of LCS\_LENGTH() and PRINT-LCS() can be shown as:\\\\
    \subfile{SOLN_TABLE.tex}
    \section{Complexity}
    \begin{itemize}
        \item Each entity of the table can be computed in $\mathcal{O}(1)$ time
        \item There are $|X|*|Y|$ entities to be filled. So total time complexity is O($|$X$|$·$|$Y$|$).
        \item Similarly total memory complexity is also O($|$X$|$·$|$Y$|$)
    \end{itemize}
    \chapter{Improvement}
    Unfortunately, unless something is said explicitly about input alphabet, no known optimization on time complexity is possible.
    
    \bigskip
    But if the sample space is known then following optimizations can be applied,
    \begin{itemize}
        \item For problems with a bounded alphabet size, the Method of Four Russians can be used to reduce the running time of the dynamic programming algorithm by a logarithmic factor.
        \item For problems where $S$ and $T$ has no repeated alphabet, Longest common subsequence problem can be reduced to Longest increasing subsequence. Thus solving the problem in $O(n\cdot logn)$.
    \end{itemize}
    \bigskip
    But memory optimization is certainly possible. In each state we
    only need last two rows. So we can keep last two rows using even,
    odd sequence. Thus giving memory complexity O(2·$|$Y$|$)
    
    \chapter{CONCLUSION}{
        \textit{Elements of Style} is a unique book on rules for composition writing. it gives in brief space the principal requirements of plain English style and concentrates attention on the rules of usage and principles of composition most commonly violated.
        
        \bigskip
        
        Despite its brief and well-structured features, this book is not completely well-explained on all topics. A little more rich collection of examples can make this book more helpful for the readers and writers.
        
        \bigskip
        
        As a basic textbook for learning composition writing and overcoming common mistakes, this book is a very good choice.
        
        \bigskip
        
        For any query, please contact : 
        \href{http://aniksarkerbuet1997@gmail.com}{\textcolor{blue}{aniksarkerbuet1997@gmail.com}}
    }
    
    \end{document}
    